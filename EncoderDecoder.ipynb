{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff9e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "989ab0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, causal=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.key = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.query = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k*n_heads)\n",
    "        \n",
    "        self.fc = nn.Linear(d_k*n_heads, d_model)\n",
    "        \n",
    "        self.causal = causal\n",
    "        \n",
    "        if causal:\n",
    "            cm = torch.tril(torch.ones(max_len, max_len))\n",
    "            self.register_buffer(\n",
    "                \"causal_mask\",\n",
    "                cm.view(1, 1, max_len, max_len)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, q, k, v, pad_mask=None):\n",
    "        q = self.query(q)\n",
    "        k = self.key(k)\n",
    "        v = self.value(v)\n",
    "        \n",
    "        N = q.shape[0]\n",
    "        T_output = q.shape[1]\n",
    "        T_input = k.shape[1]\n",
    "        \n",
    "        q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)        \n",
    "        v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)        \n",
    "        \n",
    "        attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if pad_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(\n",
    "                pad_mask[:, None, None, :] == 0, float('-inf')\n",
    "            )    \n",
    "        \n",
    "        if self.causal:\n",
    "            attn_scores = attn_scores.masked_fill(\n",
    "                self.causal_mask[:, :, :T_output, :T_output] == 0, float('-inf')\n",
    "            )\n",
    "        \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        A = attn_weights @ v\n",
    "        \n",
    "        A = A.transpose(1, 2)\n",
    "        A = A.contiguous().view(N, T_output, self.d_k * self.n_heads)\n",
    "        \n",
    "        return self.fc(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8964b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model*4, d_model),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.ln1(x + self.mha(x, x, x, pad_mask))\n",
    "        x = self.ln2(x + self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bda464a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "        self.mha_1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=True)\n",
    "        self.mha_2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model*4, d_model),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "        x = self.ln1(dec_input + self.mha_1(dec_input, dec_input, dec_input, dec_mask))\n",
    "        x = self.ln2(x + self.mha_2(x, enc_output, enc_output, enc_mask))\n",
    "        x = self.ln3(x + self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b2ca90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        exp_term = torch.arange(0, d_model, 2)\n",
    "        div_term = torch.exp(exp_term*(-math.log(10000.0)/ d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24b89266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        transformer_blocks = [EncoderBlock(d_k, d_model, n_heads, dropout) for _ in range(n_layers)]\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, pad_mask)\n",
    "            \n",
    "        x = self.ln(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a535195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "        transformer_blocks = [DecoderBlock(d_k, d_model, n_heads, max_len, dropout_prob) for _ in range(n_layers)]\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "        x = self.embedding(dec_input)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(enc_output, x, enc_mask, dec_mask)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "230ce225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
    "        enc_output = self.encoder(enc_input, enc_mask)\n",
    "        dec_output = self.decoder(enc_output, dec_input, enc_mask, dec_mask)\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fbb0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=20_000, max_len=512, d_k=16, d_model=64, n_heads=4, n_layers=2, dropout=0.1)\n",
    "decoder = Decoder(vocab_size=10_000, max_len=512, d_k=16, d_model=64, n_heads=4, n_layers=2, dropout_prob=0.1)\n",
    "transformer = Transformer(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aa25d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(10000, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha_1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha_2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha_1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha_2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35c801a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 10000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xe = np.random.randint(0, 20_000, size=(8, 512))\n",
    "xe_t = torch.tensor(xe).to(device)\n",
    "\n",
    "xd = np.random.randint(0, 10_000, size=(8, 256))\n",
    "xd_t = torch.tensor(xd).to(device)\n",
    "\n",
    "maske = np.ones((8, 512))\n",
    "maske[:, 256:] = 0\n",
    "maske_t = torch.tensor(maske).to(device)\n",
    "\n",
    "maskd = np.ones((8, 256))\n",
    "maskd[:, 128:] = 0\n",
    "maskd_t = torch.tensor(maskd).to(device)\n",
    "\n",
    "out = transformer(xe_t, xd_t, maske_t, maskd_t)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2449a8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.6464e-01,  6.4494e-01, -1.5252e+00,  ..., -1.0332e-01,\n",
       "          -7.4267e-01,  9.3116e-01],\n",
       "         [ 1.7952e-01,  6.0200e-01,  1.5039e-01,  ..., -1.0819e+00,\n",
       "          -1.3174e-01,  4.0515e-02],\n",
       "         [ 2.4100e-01,  6.0994e-01, -1.1133e+00,  ..., -1.2980e-01,\n",
       "           3.3172e-01,  5.5905e-01],\n",
       "         ...,\n",
       "         [-8.3739e-02,  2.0665e-01, -4.9890e-01,  ..., -4.2288e-01,\n",
       "          -7.6272e-03,  4.1104e-01],\n",
       "         [ 1.7699e+00, -1.1592e-01, -1.2732e-01,  ..., -6.8193e-01,\n",
       "           7.0694e-02, -6.4769e-02],\n",
       "         [-2.7897e-01,  2.9400e-01, -1.3429e+00,  ..., -1.2498e-01,\n",
       "          -6.4072e-01,  1.2271e-01]],\n",
       "\n",
       "        [[ 3.4298e-01,  1.1201e+00, -1.2626e-01,  ..., -1.2835e+00,\n",
       "          -7.0947e-01,  6.9501e-02],\n",
       "         [ 1.3564e+00,  9.0516e-01, -4.5359e-01,  ..., -2.6563e-01,\n",
       "          -1.2096e+00,  8.8686e-01],\n",
       "         [-2.4803e-01,  9.3452e-01, -8.9503e-01,  ...,  1.4626e-01,\n",
       "          -1.2990e+00, -1.9827e-01],\n",
       "         ...,\n",
       "         [ 7.0428e-01, -2.3306e-01, -3.2864e-01,  ...,  6.6532e-02,\n",
       "          -1.8213e-01,  2.2105e-02],\n",
       "         [ 3.6962e-01,  3.2446e-01,  2.4647e-01,  ..., -1.3614e-01,\n",
       "          -4.9545e-01,  6.1335e-01],\n",
       "         [ 4.3149e-01,  1.0489e+00, -2.7592e-01,  ...,  1.4725e-01,\n",
       "          -7.8538e-01,  4.6306e-01]],\n",
       "\n",
       "        [[ 6.5408e-01,  1.3432e+00, -3.8143e-01,  ..., -9.2404e-01,\n",
       "          -3.6848e-01,  7.7528e-01],\n",
       "         [-2.1647e-01,  1.7623e+00, -1.3921e+00,  ..., -9.6191e-01,\n",
       "           4.5376e-02,  8.7680e-01],\n",
       "         [ 8.9403e-01,  1.3014e+00, -4.9034e-02,  ...,  3.9535e-01,\n",
       "          -5.7412e-01, -1.2995e-01],\n",
       "         ...,\n",
       "         [-4.5775e-02,  3.4993e-01,  4.0646e-01,  ..., -1.0173e+00,\n",
       "          -5.7762e-01, -1.0596e-02],\n",
       "         [ 1.0429e+00, -3.0242e-01,  7.6706e-01,  ...,  1.7932e-01,\n",
       "          -4.7530e-01, -2.0949e-02],\n",
       "         [ 1.0390e+00,  3.7648e-01, -5.8587e-01,  ..., -2.3392e-01,\n",
       "          -2.9493e-01,  3.1874e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.6584e-01,  1.0725e+00,  9.4841e-01,  ..., -1.3540e+00,\n",
       "           6.1540e-01,  4.2733e-01],\n",
       "         [ 2.6371e-01,  8.3220e-01, -7.0026e-01,  ..., -5.7550e-01,\n",
       "          -3.0933e-01, -1.1549e-01],\n",
       "         [ 1.3124e-01,  2.1443e-01, -4.2179e-01,  ...,  2.2445e-01,\n",
       "          -9.1406e-01,  8.4079e-02],\n",
       "         ...,\n",
       "         [ 1.7577e+00,  8.3791e-01, -1.6890e-01,  ..., -5.9735e-01,\n",
       "           1.8843e-01,  9.1835e-01],\n",
       "         [ 1.1170e+00,  6.2219e-01, -2.1367e-01,  ..., -1.2834e+00,\n",
       "          -1.3639e-01,  9.6549e-01],\n",
       "         [-4.7842e-01, -4.0833e-04, -5.6363e-01,  ...,  4.0582e-01,\n",
       "          -5.4850e-01, -5.4900e-01]],\n",
       "\n",
       "        [[-1.2811e+00,  2.9142e-01, -3.4902e-01,  ...,  1.6288e-02,\n",
       "          -2.2812e-01, -4.4063e-01],\n",
       "         [-1.3152e-03,  7.5876e-01,  3.9172e-02,  ..., -3.3035e-01,\n",
       "          -8.3699e-01,  1.2064e-01],\n",
       "         [-3.0432e-01,  8.3350e-01,  3.3096e-01,  ..., -7.0798e-01,\n",
       "           4.3562e-01, -1.1157e-01],\n",
       "         ...,\n",
       "         [-7.4588e-02,  4.2972e-01, -7.0893e-01,  ...,  1.9540e-01,\n",
       "          -1.8668e-01,  8.8779e-01],\n",
       "         [ 1.8492e-01,  2.1422e-01,  1.7630e-01,  ...,  2.0039e-01,\n",
       "          -5.4547e-01,  3.5525e-01],\n",
       "         [-5.3794e-02,  2.8400e-01,  6.3567e-01,  ..., -1.2099e-01,\n",
       "          -6.7819e-01,  7.2632e-02]],\n",
       "\n",
       "        [[-5.7389e-01,  3.1743e-01, -1.0798e+00,  ...,  2.2051e-01,\n",
       "           4.4599e-01,  2.4507e-01],\n",
       "         [ 1.4261e+00,  8.4407e-01, -8.7973e-01,  ..., -5.2100e-01,\n",
       "          -7.7715e-01,  7.7772e-01],\n",
       "         [ 9.1866e-01,  1.0774e+00, -2.4539e-01,  ..., -8.8889e-01,\n",
       "          -5.8966e-01, -3.2580e-01],\n",
       "         ...,\n",
       "         [ 4.6053e-01,  7.4020e-01, -8.3594e-01,  ...,  1.7131e-01,\n",
       "          -8.7621e-01, -2.1568e-01],\n",
       "         [ 7.2277e-01, -3.6214e-01, -6.8519e-01,  ..., -2.6326e-01,\n",
       "          -5.8794e-01,  8.8226e-01],\n",
       "         [ 7.5611e-01,  1.1068e-01, -7.3332e-01,  ...,  5.0650e-01,\n",
       "           3.1551e-01,  4.9412e-01]]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b72fd33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 1615k    0 1615k    0     0   795k      0 --:--:--  0:00:02 --:--:--  795k\n",
      "100 3505k    0 3505k    0     0  1156k      0 --:--:--  0:00:03 --:--:-- 1156k\n",
      "100 5378k    0 5378k    0     0  1333k      0 --:--:--  0:00:04 --:--:-- 1334k\n",
      "100 7263k    0 7263k    0     0  1442k      0 --:--:--  0:00:05 --:--:-- 1457k\n",
      "100 7633k    0 7633k    0     0  1459k      0 --:--:--  0:00:05 --:--:-- 1856k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://lazyprogrammer.me/course_files/nlp3/spa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b678b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corre!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0        1\n",
       "0   Go.      Ve.\n",
       "1   Go.    Vete.\n",
       "2   Go.    Vaya.\n",
       "3   Hi.    Hola.\n",
       "4  Run!  ¡Corre!"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('spa.txt', sep='\\t', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f20c048a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115245, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87d49dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:30_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79c4ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['en', 'es']\n",
    "df.to_csv('spa.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6bc89493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbd5abce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1056a42d9442cf9de42af0dd04b736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset('csv', data_files='spa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2cd6ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'es'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "051ff953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'es'],\n",
       "        num_rows: 21000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'es'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = raw_dataset['train'].train_test_split(test_size=0.3, seed=42)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4116bfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d27ccc8dec049c69d46560ce1c4aa4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T470P\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\T470P\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-es. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4d9b2b1c504453a9632abe6f0f7139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dee457736984967bf6dd7b03f7740e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b6a50df09b4a81829a52c4d4656239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028417b392eb48bea75d89195054e044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = 'Helsinki-NLP/opus-mt-en-es'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4c32a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Yo', '▁puedo', '▁arreglarlo', '.', '</s>']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split['train'][0]['en']\n",
    "es_sentence = split['train'][0]['es']\n",
    "\n",
    "inputs = tokenizer(en_sentence)\n",
    "targets = tokenizer(text_target=es_sentence)\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(targets['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eec31ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yo puedo arreglarlo.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9270a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def process_function(batch):\n",
    "    model_inputs = tokenizer(batch['en'], max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(text_target=batch['es'], max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e7a43ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18933280d79d4586955ae6c648a2dda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f523485ee94b48428fedbefc9d674c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = split.map(\n",
    "    process_function,\n",
    "    batched=True,\n",
    "    remove_columns=split['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89bc3301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f1dfdfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(0, 5)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f3b27f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   33,    88,  9222,    48,     3,     0, 65000, 65000],\n",
       "        [  552, 11490,     9,   310,   255,     3,     0, 65000],\n",
       "        [  143,    31,   125,  1208,     3,     0, 65000, 65000],\n",
       "        [ 1093,   220,  1890,    23,    48,     3,     0, 65000],\n",
       "        [  124,    20,   100, 18422,    48,   141,     3,     0]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "011830dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 65000]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe1d1ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', '<unk>', '<pad>']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f52cbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "502706f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'cls_token':'<s>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b9f446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocab_size=tokenizer.vocab_size + 1,\n",
    "    max_len=512,\n",
    "    d_k=16,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size=tokenizer.vocab_size + 1,\n",
    "    max_len=512,\n",
    "    d_k=16,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    dropout_prob=0.1\n",
    ")\n",
    "\n",
    "transformer = Transformer(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "57c0eda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(65002, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha_1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha_2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha_1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (mha_2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=65002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fb37fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547dd9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# A function to encapsulate the training loop\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "\n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # move data to GPU (enc_input, enc_mask, translation)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            enc_input = batch['input_ids']\n",
    "            enc_mask = batch['attention_mask']\n",
    "            targets = batch['labels']\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
